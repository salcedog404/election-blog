---
title: "newspaper scrape"
author: "kiara hernandez"
date: '2022-10-18'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("dotenv")
# install.packages("jsonlite")
library(dotenv)
library(jsonlite)
library(tidyverse)
library(lubridate)
```

```{r}
# load up hidden api key
article_api <- "zbNGtPzlJflHASLOR1jGJDAngZjfCuNK"
#semantic_api <- Sys.getenv("SEMANTIC_API")

# set base url
base_url_art <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fq="
#base_url_sem <- "http://api.nytimes.com/svc/semantic/v2/concept/name"w

# set parameters
term <- "dobbs"
facet_field <- "day_of_week"
facet <- "true"
begin_date <- "20220101"
end_date <- "20221015"
complete_url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=dobbs&facet_field=day_of_week&facet=true&begin_date=20220101&end_date=20221015&api-key=zbNGtPzlJflHASLOR1jGJDAngZjfCuNK"


complete_url2 <-paste0(base_url_art,fq =term,"&facet_field=",facet_field,"&facet=",facet,"&begin_date=",begin_date,"&end_date=",end_date,"&api-key=",article_api,sep = "")

# import dataset to R
sus <- fromJSON(complete_url2) 

# view how many hits
sus$response$meta$hits

hits <- sus$response$meta$hits
cat("There were ",hits," hits for the search term Dobbs during 2022 to date",sep = "")

max_pages <- round((hits / 10) - 1)

# store all pages in list
pages <- list()
for(i in 0:max_pages){
    sus_df <- fromJSON(paste0(complete_url2, "&page=", i),
    flatten = TRUE) %>% 
    data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- sus_df
  Sys.sleep(6)
}


# trying again - WORKS!!!
sus0 <- fromJSON(paste0(complete_url2, "&page=0"), flatten = TRUE)
nrow(sus0$response$docs)
sus1 <- fromJSON(paste0(complete_url2, "&page=1"), flatten = TRUE)
nrow(sus1$response$docs)
sus2 <- fromJSON(paste0(complete_url2, "&page=2"), flatten = TRUE)
nrow(sus2$response$docs)

organizations <- rbind_pages(
  list(sus0$response$docs, sus1$response$docs, sus2$response$docs)
)
nrow(organizations)

pages <- list()
Sys.sleep(1) 
for(i in 0:24){
  mydata <- fromJSON(paste0(complete_url2, "&page=", i))
  message("Retrieving page ", i)
  pages[[i+1]] <- mydata$response$docs
  Sys.sleep(6) 
}

#combine all into one
organizations <- rbind_pages(pages)

#check output
nrow(organizations)

colnames(organizations)


# trying with hits
sus0 <- fromJSON(paste0(complete_url2, "&page=0"), flatten = TRUE)
nrow(sus0$response)
sus1 <- fromJSON(paste0(complete_url2, "&page=1"), flatten = TRUE)
nrow(sus1$response$docs)
sus2 <- fromJSON(paste0(complete_url2, "&page=2"), flatten = TRUE)
nrow(sus2$response$docs)

organizations <- rbind_pages(
  list(sus0$response, sus1$response, sus2$response)
)
nrow(organizations)

pages <- list()
Sys.sleep(1) 
for(i in 0:24){
  mydata <- fromJSON(paste0(complete_url2, "&page=", i)) 
  message("Retrieving page ", i)
  pages[[i+1]] <- mydata$response$docs
  Sys.sleep(6) 
}

pages <- as.data.frame(pages)
do.call(rbind.data.frame, pages)
library (plyr)
pages <- ldply(pages, data.frame)
data.frame(t(sapply(pages,c)))
rbind.fill(pages)


#combine all into one
mydata <- rbind_pages(pages)

#check output
nrow(mydata)

# save df
saveRDS(mydata, file = "dobbs_2022.RDS")

# reload
mydata <- readRDS("dobbs_2022.RDS")

# check colnames
colnames(mydata)

# visualization by month
library(dplyr)
mydata %>% 
  group_by(month = month(pub_date, label = T)) %>% 
  dplyr::summarize(count = n()) %>% 
  ggplot(aes(month, count, group = 1, color = count)) +
    geom_line() +
    labs(y = "Article Count", x = "",
         title = "NYT Articles mentioning Dobbs Supreme Court decision in 2022",
         color = "")

# visualization by day
mydata %>% 
  group_by(month_day = paste0(month(pub_date, label = T),
           day = day(pub_date))) %>% 
  dplyr::summarize(count = n()) %>% 
  ggplot(aes(month_day, count, group = 1, color = count)) +
    geom_line() +
    labs(y = "Article Count", x = "",
         title = "NYT Articles mentioning Dobbs Supreme Court decision in 2022",
         color = "")

# how about visualization by week
# extract raw date
mydata <- mydata %>% 
  mutate(publ_date = substr(pub_date, 1, 10))
head(mydata$publ_date)

# mutate week variable
mydata <- mydata %>% 
  mutate(week = strftime(publ_date, format = "%V"))
head(mydata$week)

# plot
mydata %>% 
  group_by(week) %>% 
  dplyr::summarize(count = n()) %>% 
  ggplot(aes(week, count, group = 1, color = count)) +
    geom_line() +
    labs(y = "Article Count", x = "Week",
         title = "NYT Articles mentioning Dobbs Supreme Court decision in 2022",
         color = "") + # now add line for when decision was leaked
      geom_segment(x=("18"), xend=("18"),y=0,yend=37, lty=2, color="purple", alpha=0.4) +
      annotate("text", x=("18"), y=35, label="Decision leaked", size=3) +
  geom_segment(x=("25"), xend=("25"),y=0,yend=37, lty=2, color="red", alpha=0.4) +
      annotate("text", x=("25"), y=35, label="Decision released", size=3) # now add line for when decision was actually made

# now compare this to generic ballot
X538_generic_ballot_averages_2018_2022 <- read_csv("~/Dropbox/ElectionAnalytics_Midterms/Lab sessions/Polling/Section data/final files/538_generic_ballot_averages_2018-2022.csv")
gb <- X538_generic_ballot_averages_2018_2022

# convert dat
gb <- gb %>%
  mutate(date_ = mdy(date)) %>%
  mutate(year = substr(date_, 1, 4)) %>%
  filter(year == 2022) %>%
  mutate(week = strftime(date_, format = "%V")) # Jan 1 looks weird 

# get avg by party and week
dem <- gb %>%
  filter(candidate == 'Democrats')
x <- plyr::ddply(dem, .(week), function(z) mean(z$pct_estimate))
x$candidate <- c('Democrats')
x$avg_dem <- x$V1
x <- x %>%
  select(-V1)
x$avg_dem <-  round(x$avg_dem , digits = 1)


rep <- gb %>%
  filter(candidate == 'Republicans')
y <- ddply(rep, .(week), function(z) mean(z$pct_estimate))
y$candidate <- c('Republicans')
y$avg_rep <- y$V1
y <- y %>%
  select(-V1) 
y$avg_rep <-  round(y$avg_rep, digits = 1)

#put all data frames into list
df_list <- list(gb, x, y)      

#merge all data frames together
polls_df <- df_list %>% reduce(full_join, by=c("candidate", "week"))

# remove NAs
polls_df[] <-  t(apply(polls_df, 1, function(x) c(x[!is.na(x)], x[is.na(x)])))

polls_df <- polls_df %>%
  select(-avg_rep) 

polls_df$avg_support <- polls_df$avg_dem

polls_df <- polls_df %>%
  select(-avg_dem) 

# keep only unique dates
polls_df <- polls_df %>%
  distinct(cycle, week, date_, avg_support, candidate) %>%
  filter(week != 52)

# visualize polls
polls_df %>%
  #group_by(candidate == 'Democrats') %>%
  #mutate(date_ = as.Date(date_)) %>%
  ggplot(aes(x = week, y = avg_support,
             colour = candidate)) +
  geom_line(aes(group=candidate), size = 0.3) + geom_point(size = 0.3) +
    #scale_x_date(date_labels = "%b, %Y") +
  ylab("generic ballot support") + xlab("week") +
    theme_classic() + 
  # now add line for when decision was leaked and released
      geom_segment(x=("18"), xend=("18"),y=0,yend=33, lty=2, color="purple", alpha=0.4) +
      annotate("text", x=("18"), y=31, label="Decision leaked", size=2) +
  geom_segment(x=("25"), xend=("25"),y=0,yend=33, lty=2, color="red", alpha=0.4) +
      annotate("text", x=("25"), y=31, label="Decision released", size=2)
```

