---
title: Blog Post 3 - Polling
author: Giovanni Salcedo
date: '2022-09-26'
slug: []
categories: []
tags: []
summary: In this post, I explore the predictive power of polls and update my economic fundamentals forecast model from last week with polling data.
---

*This blog is part of a series for Gov 1347: Election Analytics, a course at [Harvard University](https://www.harvard.edu/) taught by Professor [Ryan D. Enos](http://ryandenos.com/)*.

# Introduction

This week, we took a look at the role that polling plays in election forecasting. Polls of all kinds are conducted weeks, months, or even years before an election, but as we can see in [this graph](https://projects.fivethirtyeight.com/polls/generic-ballot/2022/) of FiveThirtyEight's aggregation of generic ballot polls for the 2022 midterms, polls are much more frequently conducted as election day gets closer.

As Gelman and King (1993) argue, the preferences of voters reported in polls throughout the campaign are not "informed" nor "rational," with voters instead making a decision on who to vote for based on their "enlightened preferences" formed throughout the campaign and other cues like ideology and party identification. Thus, when updating last week's models that only incorporated economic variables - CPI and unemployment - it will be interesting to see whether fundamentals alone or polls combined with fundamentals will be more predictive of this election.

# What Do Forecasters Do?

Before I begin tinkering with my model, I want to compare and contrast two well-known election forecasters and their approaches to modeling the election.

FiveThirtyEight's model, created by Nate Silver, has a [very detailed methodology page.](https://fivethirtyeight.com/methodology/how-fivethirtyeights-house-and-senate-models-work/) In summary, their model takes in as many local polls as possible, making different kinds of adjustments to them and incorporating the fundamentals to produce forecasts for each race after thousands of simulations. Their model is split into three versions: a "lite" model that only incorporates adjusted district-by-district polls and CANTOR (a system that infers polls in districts with little to no actual polling); a "classic" model that combines the lite model with fundamentals, non-polling factors like fundraising and past election results; and a "deluxe" model that combines the classic model with the race ratings by "experts", namely the Cook Political Report, Inside Elections, and Sabato's Crystall Ball. FiveThirtyEight uses an algorithm to weigh polls based on their sample size, their recency, and the pollster rating, with an emphasis on having a diversity of polls. They do note that their House model, unlike their Senate and governor models, is less polling-centric since polling for House races is more sporadic and potentially unreliable, making fundamentals a more significant factor where House polling is sparse.

The [Economist's model](https://www.economist.com/interactive/us-midterms-2022/forecast/house/how-this-works) seems to weigh polling somwehat more, though it still most definitely takes fundamentals and other factors into account. The model first tries to predict a range of outcomes for the national House popular vote, using generic ballot polls (which FiveThirtyEight use more as adjustments for local polls), presidential approval ratings, the average results of special elections held to fill vacant legislative seats, and the number of days left until the election. The model then tries to measure each district's partisan lean, incorporating more fundamentals and interestingly, an adjustment for political polarization. Finally, the model incorporates local polling, adjusting polls as needed, and simulates the election tens of thousands of times, coming up with a final prediction.

Which model do I prefer? In my opinion, both models are very well-thought out, but I prefer FiveThirtyEight's model somewhat more. I like that their model is set up into three versions, which help give readers an idea of the individual impact of polls and fundamentals as well as the wisdom of respected expert forecasters. I also prefer how their model is probabilistic, which means they are not forgetting about all the uncertainty that comes with these elections!

# Updating My Model

Last week, my models based on economic variables did not perform too well. For the comparison against polling, one change I would like to make is combining CPI and unemployment into one "economic fundamentals" model that will ideally perform better than each variable individually. Last week, I also experimented with leaving out presidential election years, but for this post, I will be considering all election years.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
## Load packages
library(tidyverse)
library(readr)

## Load data
# House and seat share by year
house_dat <- read_csv("~/Documents/R-Projects/election-blog/Labs/Lab 2/house_popvote_seats.csv")
# National unemployment by quarter
unemployment <- read_csv("~/Documents/R-Projects/election-blog/Labs/Lab 2/unemployment_national_quarterly_final.csv")
# Polls
polls_df <- read_csv("~/Documents/R-Projects/election-blog/Labs/Lab 3/polls_df.csv")
fivethirtyeight_polls <- read_csv("~/Documents/R-Projects/election-blog/Labs/Lab 3/538_house_polls_2022.csv")

# CPI by month
CPI_monthly <- read_csv("~/Documents/R-Projects/election-blog/Labs/Lab 2/CPI_monthly.csv")
# Create year and month columns based on DATE column
CPI_monthly2 <- CPI_monthly %>% 
  mutate(year = substr(DATE, 1, 4),
         month = substr(DATE, 6, 7))

## Change year and month column classes to numeric
CPI_monthly2$year <- as.numeric(CPI_monthly2$year)
CPI_monthly2$month <- as.numeric(CPI_monthly2$month)

## Calculating average percent change in CPI in last 3 months before election
CPI_monthly3 <- CPI_monthly2 %>% 
  filter(month %in% c(8, 9, 10)) %>%
  # Monthly percent change
  mutate(cpi_pct_chg = (CPIAUCSL/lag(CPIAUCSL) - 1) * 100) %>%
  # Average percent change per year between August and October
  group_by(year) %>% 
  mutate(avg_pct_chg = mean(cpi_pct_chg)) %>% 
  # Only need one row per year
  filter(month == 10)
```

```{r}
## Fundamentals-only model
# Merge all data
dat_fun <- CPI_monthly3 %>% 
  left_join(house_dat, by = 'year') %>% 
  # Filter for only election years
  drop_na() %>% 
  left_join(unemployment, by = 'year') %>% 
  select('year', 'winner_party', 'H_incumbent_party', 'H_incumbent_party_majorvote_pct', 'avg_pct_chg', 'quarter_yr', 'UNRATE') %>% 
  # Filter for unemployment one month before election
  filter(quarter_yr == 4)

## Linear model
lm_fun <- lm(H_incumbent_party_majorvote_pct ~ avg_pct_chg + UNRATE, data = dat_fun)
summary(lm_fun)
```
Above is a quick summary of my new fundamentals-only model. In this model, the unemployment rate one month before the election (but not the average percent change in CPI in the last 3 months before the election) appears to have a statistically significant positive effect on the house incumbent party voteshare.

```{r}
## Mean-squared error
mse_g <- mean((lm_fun$model$H_incumbent_party_majorvote_pct-lm_fun$fitted.values)^2)
sqrt(mse_g)
```

In terms of in-sample fit, the above number represents the mean squared error, which on its own does not mean much, but we will use it to this model to the other models. The adjusted R-squared is 0.1594, which is better than most of my models from last week, but it is still relatively low.

```{r}
## Polls + fundamentals model
dat_poll_fun <- polls_df %>%
  group_by(year, party) %>% 
  mutate(avg_support = mean(support)) %>% 
  ungroup() %>% 
  left_join(dat_fun, by = 'year') %>% 
  filter(party == "D")

## Linear model
lm_poll_fun <- lm(H_incumbent_party_majorvote_pct ~ avg_pct_chg + UNRATE + avg_support, data = dat_poll_fun)
summary(lm_poll_fun)
```

Above is a quick summary of the combined polls and fundamentals model. All of the variables are statistically significant!

```{r}
## Mean-squared error
mse_g2 <- mean((lm_poll_fun$model$H_incumbent_party_majorvote_pct-lm_fun$fitted.values)^2)
sqrt(mse_g2)
```

The mean squared error is higher for this model, but based on the warning message, this may not be an accurate measure of in-sample fit. The adjusted R-squared for this model is 0.2833, which is significantly higher than the other model's value of 0.1594, making this model have a better in-sample fit.

# Comparing Models

```{r}
## Fundamentals-only model prediction plot
ggplot(dat_fun, aes(x = predict(lm_fun), y = H_incumbent_party_majorvote_pct,
                    label = year)) +
  geom_text() +
  geom_abline(intercept=0, slope=1, color = "red") +
  labs(title = "Fundamentals-only - Predicted vs. Observed Incumbent Party PV",
       x='Predicted PV',
       y='Observed PV') +
  theme_bw()

## Subset average of most recently available (in the data) three months of this year's CPI
CPI_2022 <- CPI_monthly2 %>%
  filter(year == 2022 & month %in% c(3, 4, 5, 6)) %>% 
  # Monthly percent change
  mutate(cpi_pct_chg = (CPIAUCSL/lag(CPIAUCSL) - 1) * 100) %>%
  drop_na() %>% 
  # Average percent change per year between August and October
  group_by(year) %>% 
  mutate(avg_pct_chg = mean(cpi_pct_chg)) %>% 
  filter(month == 6) %>% 
  ungroup() %>% 
  select(avg_pct_chg) %>% 
  as.numeric()

## Subset most recent unemployment data (April 2022)
ue_2022 <- unemployment %>% 
  filter(year_qt == "2022_2") %>% 
  select(UNRATE) %>% 
  as.numeric()
```

```{r}
## Polls and fundamentals model prediction plot
ggplot(dat_poll_fun, aes(x = predict(lm_poll_fun), y = H_incumbent_party_majorvote_pct,
                    label = year)) +
  geom_text() +
  geom_abline(intercept=0, slope=1, color = "blue") +
  labs(title = "Polls + Fundamentals - Predicted vs. Observed Incumbent Party PV",
       x='Predicted PV',
       y='Observed PV') +
  theme_bw()

## Subset 2022 average Dem support
polls_2022 <- fivethirtyeight_polls %>% 
  filter(election_date == "11/8/22",
         party == "DEM") %>% 
  summarize(avg_support = mean(pct)) %>% 
  as.numeric()
```

The above are predicted vs. observed incumbent party voteshares for each of my models. Although neither are necessarily good fits for the data, both models, particularly the polls + fundamentals models, are an improvement over last week's models!


# 2022 Predictions

Fundamentals-only
```{r}
predict(lm_fun, data.frame(avg_pct_chg = 0.8759928, UNRATE = 3.6), interval = "prediction")
```

Polls + Fundamentals
```{r}
predict(lm_poll_fun, data.frame(avg_pct_chg = 0.8759928, UNRATE = 3.6,
                                avg_support = 42.7275833333333), interval = "prediction")
```

Above are the predictions each model has made for the incumbent party voteshare for the 2022 midterms. The fundamentals-only model predicts Democrats will win about 50.01% of the vote, while the polls + fundamentals model suggests Democrats will fall short of winning the majority of the popular vote with about 48.25% of the vote. The latter model presents the most pessimistic outcome for Democrats out of all of my models so far. Next week, I am interested to see if and how this will change.

# References

Gelman, A., & King, G. (1993). Why Are American Presidential Election Campaign Polls So Variable When Votes Are So Predictable? British Journal of Political Science, 23(4), 409-451. doi:10.1017/S0007123400006682